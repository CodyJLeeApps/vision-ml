# vision-ml
This application works as a Vision MachineLearning app to determine the main item within a photo.
The user will open the app, frame the photo within the live preview (video) screen, and tap the screen to take the photo and see the results.

Once the result is determined, it will be displayed on the screen, as well as spoken by the speech synthesizer.

## Core Technologies
This application uses the following technologies:
* CoreML - Enables Machine Learning Models, which can be found: [CoreML Models](https://developer.apple.com/machine-learning/)
* Vision - Enables high performance image analysis: [Vision Framework](https://developer.apple.com/documentation/vision)
* Speech Synthesis - Enables speech playback for hearing empaired and other applications. [AVSpeechSynthesis](https://developer.apple.com/documentation/avfoundation/avspeechsynthesizer)

## Example Application Screenshots

Below are example video and screenshots of the vision-ml app flow.

[vision-ml Example Video](https://vimeo.com/user72345907/prototype-vision-ml)

![defaultView-remote](https://github.com/CodyJLeeApps/vision-ml/blob/master/app_screenshots/defaultView-remote.PNG)
![defaultView-tree](https://github.com/CodyJLeeApps/vision-ml/blob/master/app_screenshots/defaultView-tree.PNG)

![gameRemote-confident](https://github.com/CodyJLeeApps/vision-ml/blob/master/app_screenshots/gameRemote-confident.PNG)
![mouse-confident](https://github.com/CodyJLeeApps/vision-ml/blob/master/app_screenshots/mouse-confident.PNG)

![unsure](https://github.com/CodyJLeeApps/vision-ml/blob/master/app_screenshots/rubiks-unsure.PNG)